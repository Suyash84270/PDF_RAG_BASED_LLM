# RAG Chain Q&A App

## ğŸ“Œ Overview
The **RAG Chain Q&A App** is an advanced **Retrieval-Augmented Generation (RAG)** application that enables users to upload a PDF document and ask contextual questions based on its content. This app uses **LangChain, FAISS, and Ollama's deepseek-r1 model** to retrieve relevant document sections and generate accurate answers in real-time.
![1](https://github.com/user-attachments/assets/5265e27b-9265-4f1a-9037-dc0d64f7c081)



![22](https://github.com/user-attachments/assets/0c0cbda2-8c58-45aa-b1c4-7d94f36a26fc)


![3](https://github.com/user-attachments/assets/b0afde73-fbee-441b-bff1-b433ba637372)



## âœ¨ Features
- ğŸ“„ **Document Upload:** Upload a PDF document and extract its content into Markdown format.
- ğŸ” **Intelligent Chunking:** Split the document into structured sections based on headers.
- ğŸ§  **Vector Store & Embeddings:** Convert document chunks into embeddings and store them in FAISS for efficient retrieval.
- ğŸ¤– **RAG-based Q&A:** Leverage a retrieval-augmented generation pipeline to answer user queries using the uploaded document.
- ğŸš€ **Real-time Answer Streaming:** Display responses dynamically as they are generated.

## ğŸ—ï¸ Tech Stack
- **Frontend:** Streamlit (for interactive UI)
- **Backend:** LangChain, FAISS, Ollama
- **Embedding Model:** `nomic-embed-text`
- **LLM Model:** `deepseek-r1:1.5b`
- **Document Processing:** `docling.document_converter`

## ğŸ“‚ Project Structure
```
ğŸ“ rag-qa-app
â”œâ”€â”€ ğŸ“„ app.py            # Main Streamlit application
â”œâ”€â”€ ğŸ“„ requirements.txt  # Dependencies list
â”œâ”€â”€ ğŸ“„ README.md         # Project documentation
          
```

## ğŸ› ï¸ Setup and Installation

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-repo/rag-qa-app.git
cd rag-qa-app
```

### 2ï¸âƒ£ Create a Virtual Environment & Install Dependencies
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`
pip install -r requirements.txt
```

### 3ï¸âƒ£ Start Ollama (Ensure it's Running Locally)
```bash
ollama serve
```

### 4ï¸âƒ£ Run the Application
```bash
streamlit run app.py
```

## ğŸ–¥ï¸ Usage Guide
1ï¸âƒ£ **Upload a PDF document** in the sidebar.
2ï¸âƒ£ The document is **converted to Markdown and split into structured chunks**.
3ï¸âƒ£ A **vector store is created** with FAISS for fast retrieval.
4ï¸âƒ£ **Ask a question** in the text input box.
5ï¸âƒ£ The system retrieves relevant document chunks and generates an answer in **bullet points**.

## âš¡ Example Output
### **Input Question:**
> What are the main objectives of the document?

### **Generated Answer:**
- âœ… The document aims to provide an overview of...
- âœ… Key objectives include...
- âœ… The author highlights...

## ğŸ“Œ Future Improvements
- [ ] Support for additional document formats (e.g., DOCX, TXT)
- [ ] Multi-document support
- [ ] More advanced retrieval mechanisms
- [ ] Fine-tuned LLM for specific domain-based Q&A

## ğŸ¤ Contributing
Contributions are welcome! Feel free to submit issues, pull requests, or suggestions.

## ğŸ“œ License
This project is licensed under the **MIT License**.

## ğŸŒŸ Acknowledgments
- **LangChain** for easy orchestration of LLMs and retrieval mechanisms.
- **FAISS** for efficient document search.
- **Ollama** for providing lightweight local LLM serving.
- **Streamlit** for creating an interactive interface effortlessly.

---
ğŸš€ **Transform the way you interact with documents using AI!**

